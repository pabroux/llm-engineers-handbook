[project]
authors = [{name = "pabroux", email = "pierre-alexandrebroux@live.fr"}]
name = "llm-twin"
requires-python = ">= 3.11"
version = "0.1.0"
dependencies = ["pymongo>=4.6.2", "click>=8.0.1", "loguru>=0.7.2", "rich>=13.7.1", "numpy>=1.26.4", "datasets>=3.0.1", "torch>=2.2.2", "zenml[server]==0.74.0", "selenium>=4.21.0", "webdriver-manager>=4.0.1", "beautifulsoup4>=4.12.3", "html2text>=2024.2.26", "jmespath>=1.0.1", "chromedriver-autoinstaller>=0.6.4", "qdrant-client>=1.8.0", "langchain>=0.2.11", "sentence-transformers>=3.0.0", "langchain-openai>=0.1.3", "jinja2>=3.1.4", "tiktoken>=0.7.0", "fake-useragent>=1.5.1", "langchain-community>=0.2.11", "fastapi>=0.110.0,<0.111", "uvicorn>=0.30.6", "opik>=0.2.2"]

[build-system]
build-backend = "hatchling.build"
requires = ["hatchling"]

[tool.pixi.workspace]
channels = ["conda-forge"]
platforms = ["osx-arm64"]

[tool.pixi.pypi-dependencies]

[tool.pixi.tasks]
# Data pipelines
run-digital-data-etl-maxime = "pixi run python -m tools.run --run-etl --no-cache --etl-config-filename digital_data_etl_maxime_labonne.yaml"
run-digital-data-etl-paul = "pixi run python -m tools.run --run-etl --no-cache --etl-config-filename digital_data_etl_paul_iusztin.yaml"
run-digital-data-etl = [
    { task = "run-digital-data-etl-maxime" },
    { task = "run-digital-data-etl-paul" },
]
run-feature-engineering-pipeline = "pixi run python -m tools.run --no-cache --run-feature-engineering"
run-generate-instruct-datasets-pipeline = "pixi run python -m tools.run --no-cache --run-generate-instruct-datasets"
run-generate-preference-datasets-pipeline = "pixi run python -m tools.run --no-cache --run-generate-preference-datasets"
run-end-to-end-data-pipeline = "pixi run python -m tools.run --no-cache --run-end-to-end-data"

# Utility pipelines
run-export-artifact-to-json-pipeline = "pixi run python -m tools.run --no-cache --run-export-artifact-to-json"
run-export-data-warehouse-to-json = "pixi run python -m tools.data_warehouse --export-raw-data"
run-import-data-warehouse-from-json = "pixi run python -m tools.data_warehouse --import-raw-data"

# Training pipelines
run-training-pipeline = "pixi run python -m tools.run --no-cache --run-training"
run-evaluation-pipeline = "pixi run python -m tools.run --no-cache --run-evaluation"

# Inference
call-rag-retrieval-module = "pixi run python -m tools.rag"
run-inference-ml-service = "pixi run uvicorn tools.ml_service:app --host 0.0.0.0 --port 8000 --reload"
call-inference-ml-service = "curl -X POST 'http://127.0.0.1:8000/rag' -H 'Content-Type: application/json' -d '{\"query\": \"My name is Paul Iusztin. Could you draft a LinkedIn post discussing RAG systems? I am particularly interested in how RAG works and how it is integrated with vector DBs and LLMs.\"}'"

# Infrastructure
# ↳ Local infrastructure
local-docker-infrastructure-up = "docker compose up -d"
local-docker-infrastructure-down = "docker compose stop"
local-zenml-server-down = "pixi run zenml logout --local"
local-zenml-server-up = { cmd = "pixi run zenml login --local", env = { OBJC_DISABLE_INITIALIZE_FORK_SAFETY = "YES" } }
local-infrastructure-up= [
  { task = "local-docker-infra-up" },
  { task = "local-zenml-server-down" },
  { task = "local-zenml-server-up" },
]
local-infrastructure-down = [
  { task = "local-docker-infra-down" },
  { task = "local-zenml-server-down" },
]
set-local-stack = "pixi run zenml stack set default"
set-aws-stack = "pixi run zenml stack set aws-stack"
set-asynchronous-runs = "pixi run zenml orchestrator update aws-stack --synchronous=False"
zenml-server-disconnect = "pixi run zenml disconnect"

# ↳ Settings
export-settings-to-zenml = "pixi run python -m tools.run --export-settings"
delete-settings-zenml = "pixi run zenml secret delete settings"

# ↳ SageMaker
create-sagemaker-role = "pixi run python -m llm_engineering.infrastructure.aws.roles.create_sagemaker_role"
create-sagemaker-execution-role = "pixi run python -m llm_engineering.infrastructure.aws.roles.create_execution_role"
deploy-inference-endpoint = "pixi run python -m llm_engineering.infrastructure.aws.deploy.huggingface.run"
test-sagemaker-endpoint = "pixi run python -m llm_engineering.model.inference.test"
delete-inference-endpoint = "pixi run python -m llm_engineering.infrastructure.aws.deploy.delete_sagemaker_endpoint"

# ↳ Docker
build-docker-image = "docker buildx build --platform linux/amd64 -t llmtwin -f Dockerfile ."
run-docker-end-to-end-data-pipeline = "docker run --rm --network host --shm-size=2g --env-file .env llmtwin poetry poe --no-cache --run-end-to-end-data"
bash-docker-container = "docker run --rm -it --network host --env-file .env llmtwin bash"

# QA (Quality Assurance)
lint-check = "pixi run ruff check ."
format-check = "pixi run ruff format --check ."
lint-check-docker = "sh -c 'docker run --rm -i hadolint/hadolint < Dockerfile'"
gitleaks-check = "docker run -v .:/src zricethezav/gitleaks:latest dir /src/llm_engineering"
lint-fix = "pixi run ruff check --fix ."
format-fix = "pixi run ruff format ."

# Tests
test = "pixi run pytest"

[tool.pixi.dependencies]
python = "3.11.8.*"

[tool.pixi.environments]
default = { solve-group = "default" }
dev = { features = ["dev"], solve-group = "default" }
aws = { features = ["aws"], solve-group = "default" }

[dependency-groups]
dev = ["ruff>=0.4.9", "pytest>=8.2.2", "pre-commit>=3.7.1"]
aws = ["sagemaker>=2.232.2", "s3fs>=2022.3.0", "aws-profile-manager>=0.7.3", "kubernetes>=30.1.0", "sagemaker-huggingface-inference-toolkit>=2.4.0"]
